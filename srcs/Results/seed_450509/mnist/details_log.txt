Cut-off: 0.1
Network architecture:
Input layer: 31 units, activation: relu
Hidden layer: 50 units, activation: relu
Hidden layer: 50 units, activation: relu
Hidden layer: 20 units, activation: relu
Output layer: 4 units, activation: softmax