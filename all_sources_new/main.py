
import os
import sys
sys.path.append(os.getcwd() + r'\all_pkg')

import matplotlib.pyplot as plt
plt.rcParams.update(plt.rcParamsDefault)
plt.style.use('seaborn-ticks')
plt.close('all')
import pickle
import random
import numpy as np

import tensorflow as tf

import keras_ops as ko
import streams
import preprocess_kernel as prek
import motifs_postprocess as mp
import train_plots as tp


"""
Utility variables.
To define which code blocks activate, whether to produce plots

. plot    :    training. Plots of loss and accuracy trends in the training
                         stage
               distributions. The weights distributions before and after 
                              training
               network. Network weights visualisation
               preprocess. Values spectra of weights and biases
               motifs. Scatter plots, variations distributions
               
. again   :    train. If True, the training stage is repeated. If the trained
                      *.h5 models are generated yet, set to False
               initialise. To perform once for all. Set to True alongside with
                           `train' only for the first run
               train_plots.Postprocess visualisations
               preprocess. If True, then the module to plot the KDEs and histo-
                           grams for the weights spectra are plotted and if 
                           the variable write_file == 'y', then the graph file
                           to feed FANMOD is created again
               tex_source. If True, the motifs_, variations_ and most_changed_
                           serialized pandas DataFrames are loaded and a rough
                           LaTeX code is saved in the location where the LaTeX
                           sources are saved. Set this location depending on
                           the files organisation.
                           
                           ****** NOTE: still in progress. The table shall be manually
                           formatted
               summ_plots. significance profiles. Serialized pd DataFrames are
                           picked
"""


plot = {'training'      : False,
        'distributions' : False,
        'network'       : False,
        'preprocess'    : True,
        'motifs'        : True}

again = {'initialise'   : False,
         'train'        : False,
         'train_plots'  : False,
         'preprocess'   : False,
         'postprocess'  : False,
         'tex_source'   : False,
         'summ_plots'   : True,
         'efficacy'     : False}

split_fraction = 0.3

weighted_graph = 'w'
write_file = 'n'
size = 4    # size = [4,6]
detail = 'mtm'


#seeds = [3,5,6]
seeds = [618]


initialisations = ['orth','normal','glorot'] # U ['zeros'] ?
#initialisations = ['orth']
#datasets = ['init','tree','clus','mvg']
datasets = ['init','tree','clus']
#datasets = ['tree']

path_in_results = os.getcwd() + r'\Results'
path_save_figs =   # *** absolute path where figures are wanted to be saved ***
streams.check_create_directory(path_save_figs)


"""
The idea is to loop over:

    ~ seeds 
        
        ~ initialisation schemes
        
in such a way to execute the program once of all the possible configuration
Apposite directories are created, if not already present, on-fly, in such a way
to store the informations generated by the program execution.

Images are stored directly in the directory in which the written project 
is kept and worked. 
"""


for seed_value in seeds:
    
    
    os.environ['PYTHONHASHSEED'] = str(seed_value)
    random.seed(seed_value)
    np.random.seed(seed_value)
    tf.set_random_seed(seed_value)
    
    directory = '\seed_'+str(seed_value)
    path_in_seeds = path_in_results + directory
    streams.check_create_directory(path_in_seeds)
    
    """
    Note that the dictionaries here initialised are used in the 
    ~ initialisation schemes loop, in such a way to save the results 
    for each different scheme.
    """
    
    most_changed_motifs_initscheme = {}
    all_variations = {}
    all_motifs = {}
    
    for init_scheme in initialisations:
        
        print("--- ---\n Seed : {}\n Initialisation scheme : {}".format(
                seed_value,init_scheme))
        
        path_def = path_in_seeds + '\{}'.format(init_scheme)
        streams.check_create_directory(path_def)
        path_figs = path_save_figs + r'\{}'.format(init_scheme)
        streams.check_create_directory(path_figs)
        
        """
        Model initialisation and training. 
        """
        if (again['initialise']):
            dataset_id = 'init'
            model_pretrain = ko.model_initialisation(31,4, 
                                                     init_scheme, seed_value, 
                                                     path_def)
        #end
        
        if (again['train']):
            
            for dataset_id in datasets[1:]:
                if (dataset_id == 'mvg'):
                    ko.model_train_multitask(path_def, dataset_id,
                                             split_fraction, plot,
                                             init_scheme)
                else:
                    ko.model_training(path_def, dataset_id,
                                      split_fraction, plot, init_scheme)
                #end
            #end
        #end
        
        if (again['train_plots']):
            tp.distributions_plot(path_def,datasets,init_scheme)
        #end
        
        """
        Here: preprocess stage. For each initialisation scheme 
        (currently looping), all the weights of all the models, 
        i.e. initial configuration, tree, clusters, multitask, are
        gathered and the categories are created. 
        """
        
        if (again['preprocess']):
            bins_edges = prek.bins_for_scheme(path_def,datasets,init_scheme)
            edges_dfs = [prek.spectrum_discretize(path_def, dataset_id, plot,
                                      weighted_graph, write_file, 
                                      init_scheme, bins_edges)  \
                                           for dataset_id in datasets]
        #end
            
        #end
        
        if (again['postprocess']):
            """
            Postprocess
            Note that the function ``launcher_weighted_analysis'' is used for both
            weighted and unweighted analysis. In the latter case, there is only one
            instance for topological group, then regardless of the ``detail'' flag,
            that only instance is selected.
            ``offset'' flag is used for the .csv reading stage. It is motifs mining 
            program-dependent.
            
            The dictionaries 
                . all_variations
                . most_changed_motifs_initscheme
                . all_motifs
            are used to keep the track of the results for each initialisation scheme
            """
            
            path_init = path_def + r'\init\init_{}_s{}_out.csv'.format(
                                                                weighted_graph,
                                                                size)
            if (weighted_graph == 'u'):
                offset = 0                
            elif (weighted_graph == 'w'):
                offset = 2
            #end
            
            motifs, variations, most_changed =  \
                        mp.launcher_weighted_analysis(path_init, path_def, 
                                                      path_figs, datasets,
                                                      offset, weighted_graph, 
                                                      size, plot, detail)
            
            all_variations.update({init_scheme : variations})
            most_changed_motifs_initscheme.update({init_scheme : most_changed})
            all_motifs.update({init_scheme : motifs})
                
            #end IF_WEIGHTED statement
            
        #end POSTPROCESS
        
    #end INIT_SCHEME loop
    
    if (again['postprocess']):
        
        """
        Here the results, in pd.DataFrame format, are serialized
        to be subsequently picked, avoiding to repeat all of the 
        computations above which are relatively time consuming.
        """
        
        if (weighted_graph == 'u'):
            append_ = r'u_s{}.pkl'.format(str(size))
        elif (weighted_graph == 'w'):
            append_ = r'w_s{}_{}.pkl'.format(str(size),detail)
        #end
        
        name = r'\most_changed_dataframes_' + append_
        fileID = open(path_in_seeds + name,'wb')
        pickle.dump(most_changed_motifs_initscheme,fileID)
        fileID.close()
        
        name = r'\motifs_variations_all_' + append_
        fileID = open(path_in_seeds + name,'wb')
        pickle.dump(all_variations,fileID)
        fileID.close()
        
        name = r'\motifs_all_' + append_
        fileID = open(path_in_seeds + name,'wb')
        pickle.dump(all_motifs,fileID)
        fileID.close()
    #end
    
    if (again['tex_source']):
        
        if (weighted_graph == 'u'):
            append_ = r'u_s{}.pkl'.format(str(size))
        elif (weighted_graph == 'w'):
            append_ = r'w_s{}_{}.pkl'.format(str(size),detail)
        #end
        
        name = r'\most_changed_dataframes_' + append_
        fileID = open(path_in_seeds + name,'rb')
        most_changed_dfs = pickle.load(fileID)
        fileID.close()
        
        name = r'\motifs_all_' + append_
        fileID = open(path_in_seeds + name,'rb')
        all_motifs = pickle.load(fileID)
        fileID.close()
        
        print("Producing LaTeX code")
        
        mp.LaTeX_source_export(most_changed_dfs,weighted_graph,size,seed_value,
                               detail, variations = True)
        mp.LaTeX_source_export(all_motifs,weighted_graph,size,seed_value,
                               detail, variations = False)
    #end
    
    if (again['summ_plots']):
        
        if (weighted_graph == 'u'):
            append_ = r'u_s{}.pkl'.format(str(size))
        elif (weighted_graph == 'w'):
            append_ = r'w_s{}_{}.pkl'.format(str(size),detail)
        #end
        
        name = r'\most_changed_dataframes_' + append_
        
        fileID = open(path_in_seeds + name,'rb')
        most_changed_dfs = pickle.load(fileID)
        fileID.close()
        
        name = r'\motifs_all_' + append_
        
        fileID = open(path_in_seeds + name,'rb')
        all_motifs = pickle.load(fileID)
        fileID.close()
        
        path_figs = # *** absolute path where figures are wanted to be saved ***
        streams.check_create_directory(path_figs + r'\summ_images')
        
        mp.significance_profiles(all_motifs,path_figs + r'\summ_images',
                                 initialisations,datasets,weighted_graph,size,
                                 plot, detail,
                                 by = 'initialisation')
        mp.significance_profiles(all_motifs,path_figs + r'\summ_images',
                                 initialisations,datasets,weighted_graph,size,
                                 plot, detail,
                                 by = 'dataset')
    #end
    
    if (again['efficacy']):
        for dataset_id in datasets[1:-1]:
            
            histories = []
            fig,ax = plt.subplots(figsize=(7.5,4))
            for init_scheme in initialisations:
                
                path_def = path_in_seeds + '\{}'.format(init_scheme)
                hist = ko.model_training(path_def,dataset_id,
                                         split_fraction,plot,init_scheme)
                histories.append(hist)
            #end
            for hist in histories:
                plt.plot(np.arange(0,10),hist[:10],lw = 2, alpha = 0.75)
                ax.set_xticks(np.arange(0,10))
                plt.xlabel('Epochs')
                plt.ylabel('Accuracy')
                plt.title('Training Efficacy')
                plt.legend(['Orthogonal','Normal','Glorot'])
            #end
            plt.show()
        #end
    #end

       
#end SEED loop



 















